{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59600db-3142-45c8-9d7a-aa07d86a6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # 넘파이 라이브러리를 가져옵니다. 수치 계산에 사용됩니다.\n",
    "import pandas as pd # 판다스 라이브러리를 가져옵니다. 데이터 조작 및 분석에 사용됩니다.\n",
    "from sklearn.preprocessing import StandardScaler # 데이터 전처리를 위해 StandardScaler를 가져옵니다. 특성 스케일링에 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9635c3e1-07bb-4335-9c0e-c17d3c5a89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오류역전파와 시그모이드 활성화 함수를 사용하는 신경망 클래스\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # 클래스 초기화 시 가중치를 무작위로 초기화합니다.\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)  # 입력층에서 은닉층으로 가는 가중치\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size) # 은닉층에서 출력층으로 가는 가중치\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # 시그모이드 활성화 함수: 신경망의 출력을 0과 1 사이로 제한합니다.\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        # 시그모이드 함수의 미분: 역전파 시 사용됩니다.\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, inputs, labels, epochs, learning_rate):\n",
    "        # 신경망을 훈련하는 함수\n",
    "        for epoch in range(epochs):\n",
    "            # 순전파 과정\n",
    "            # 입력층에서 은닉층으로의 신호 계산\n",
    "            hidden_input = np.dot(inputs, self.weights_input_hidden)\n",
    "            # 은닉층의 출력 계산 (활성화 함수 적용)\n",
    "            hidden_output = self.sigmoid(hidden_input)\n",
    "    \n",
    "            # 은닉층에서 출력층으로의 신호 계산\n",
    "            final_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "            # 출력층의 출력 계산 (활성화 함수 적용)\n",
    "            final_output = self.sigmoid(final_input)\n",
    "    \n",
    "            # 오차 계산\n",
    "            # 실제 레이블과 예측 값 간의 차이\n",
    "            error = labels - final_output\n",
    "            \n",
    "            # 역전파 과정\n",
    "            # 출력층의 오차 신호 계산\n",
    "            d_final_output = error * self.sigmoid_derivative(final_output)\n",
    "            # 은닉층의 오차 신호 계산\n",
    "            error_hidden_layer = d_final_output.dot(self.weights_hidden_output.T)\n",
    "            # 은닉층의 오차 신호에 대한 미분 값 계산\n",
    "            d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(hidden_output)\n",
    "    \n",
    "            # 가중치 업데이트\n",
    "            # 은닉층에서 출력층으로 가는 가중치 업데이트\n",
    "            self.weights_hidden_output += hidden_output.T.dot(d_final_output) * learning_rate\n",
    "            # 입력층에서 은닉층으로 가는 가중치 업데이트\n",
    "            self.weights_input_hidden += inputs.T.dot(d_hidden_layer) * learning_rate\n",
    "    \n",
    "            # 에포크마다 손실을 출력\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean(np.abs(error))\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # 신경망을 사용하여 입력 데이터에 대한 예측을 수행하는 함수\n",
    "        hidden_input = np.dot(inputs, self.weights_input_hidden)\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "\n",
    "        final_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "        final_output = self.sigmoid(final_input)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3135e4f4-ca39-49d9-9ba4-ccc6b6359f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     0    1    2    3\n",
       " 0  6.3  3.3  6.0  2.5\n",
       " 1  5.8  2.7  5.1  1.9\n",
       " 2  7.1  3.0  5.9  2.1\n",
       " 3  6.3  2.9  5.6  1.8\n",
       " 4  6.5  3.0  5.8  2.2,\n",
       "      0    1    2    3\n",
       " 0  7.2  3.2  6.0  1.8\n",
       " 1  6.2  2.8  4.8  1.8\n",
       " 2  6.1  3.0  4.9  1.8\n",
       " 3  6.4  2.8  5.6  2.1\n",
       " 4  7.2  3.0  5.8  1.6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 및 테스트 데이터 파일 경로 설정\n",
    "train_data_path = 'training.dat'  # 훈련 데이터 파일의 경로\n",
    "test_data_path = 'testing.dat'    # 테스트 데이터 파일의 경로\n",
    "\n",
    "# Pandas를 사용하여 훈련 및 테스트 데이터 파일 로드\n",
    "# 파일에 헤더가 없고 공백으로 구분된 데이터로 가정\n",
    "train_data = pd.read_csv(train_data_path, header=None, delim_whitespace=True)  # 훈련 데이터 로드\n",
    "test_data = pd.read_csv(test_data_path, header=None, delim_whitespace=True)    # 테스트 데이터 로드\n",
    "\n",
    "# 데이터의 처음 몇 줄을 출력하여 구조 확인\n",
    "# 이는 데이터에 어떤 특성이 있는지, 어떤 형태의 데이터인지 이해하기 위함\n",
    "train_data.head(), test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdab9783-1d15-4cf4-9a54-0695326f7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_data = pd.read_csv('training.dat', header=None, delim_whitespace=True)\n",
    "test_data = pd.read_csv('testing.dat', header=None, delim_whitespace=True)\n",
    "\n",
    "# 특성과 레이블 분리\n",
    "X_train = train_data.iloc[:, :-1].values  # 훈련 데이터에서 마지막 열을 제외한 모든 열을 특성으로 설정\n",
    "y_train = train_data.iloc[:, -1].values   # 훈련 데이터의 마지막 열을 레이블로 설정\n",
    "X_test = test_data.iloc[:, :-1].values    # 테스트 데이터에서 마지막 열을 제외한 모든 열을 특성으로 설정\n",
    "y_test = test_data.iloc[:, -1].values     # 테스트 데이터의 마지막 열을 레이블로 설정\n",
    "\n",
    "# 데이터 표준화\n",
    "scaler = StandardScaler()                 # StandardScaler 인스턴스 생성\n",
    "X_train = scaler.fit_transform(X_train)   # 훈련 데이터를 표준화 (평균 0, 표준편차 1)\n",
    "X_test = scaler.transform(X_test)         # 테스트 데이터를 같은 스케일로 표준화 (훈련 데이터 기준)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cf23b4c-6631-4212-920a-9d75bd73eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8927790327799806\n",
      "Epoch 1000, Loss: 0.48268799142954083\n",
      "Epoch 2000, Loss: 0.4826322707368309\n",
      "Epoch 3000, Loss: 0.4826125420789385\n",
      "Epoch 4000, Loss: 0.4826030034180308\n",
      "Epoch 5000, Loss: 0.48259893450632935\n",
      "Epoch 6000, Loss: 0.482597425863056\n",
      "Epoch 7000, Loss: 0.4825967611223472\n",
      "Epoch 8000, Loss: 0.4825956880436316\n",
      "Epoch 9000, Loss: 0.4825930780861762\n"
     ]
    }
   ],
   "source": [
    "# 신경망 인스턴스 생성\n",
    "# SimpleNeuralNetwork 클래스의 인스턴스를 생성합니다.\n",
    "# input_size는 입력층의 노드 수, hidden_size는 은닉층의 노드 수, output_size는 출력층의 노드 수를 의미합니다.\n",
    "nn = SimpleNeuralNetwork(input_size=X_train_scaled.shape[1], hidden_size=4, output_size=1)\n",
    "\n",
    "# 신경망 학습\n",
    "# 훈련 데이터의 레이블(y_train)을 신경망의 출력 형태에 맞게 조정합니다.\n",
    "# 여기서는 y_train을 (데이터 개수, 1)의 형태로 변경하여 각 데이터 포인트에 대한 하나의 출력값이 있도록 합니다.\n",
    "y_train = y_train.reshape(-1, 1)  # y_train을 (75, 1) 형태로 조정\n",
    "\n",
    "# train 메서드를 사용하여 신경망 모델을 학습시킵니다.\n",
    "# X_train_scaled는 표준화된 훈련 데이터의 특성,\n",
    "# y_train은 훈련 데이터의 레이블,\n",
    "# epochs는 학습을 진행할 총 반복 횟수,\n",
    "# learning_rate는 학습률(가중치 조정 시 적용되는 비율)을 의미합니다.\n",
    "nn.train(X_train_scaled, y_train, epochs=10000, learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91d7597a-89de-404d-8bf6-6924b6896d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.04\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대한 예측 수행\n",
    "# 신경망 모델(nn)의 'predict' 메서드를 사용하여 X_test_scaled 데이터에 대한 예측을 수행합니다.\n",
    "y_pred = nn.predict(X_test_scaled)\n",
    "\n",
    "# 신경망의 출력을 이진 레이블로 변환\n",
    "# 신경망은 확률 값을 출력하기 때문에, 이를 이진 레이블(0 또는 1)로 변환합니다.\n",
    "# 0.5를 임계값으로 설정하여, 0.5보다 큰 값은 1로, 그렇지 않은 값은 0으로 변환합니다.\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# 실제 레이블과 예측 레이블을 비교하여 정확도 계산\n",
    "# y_test는 테스트 데이터의 실제 레이블을 나타냅니다.\n",
    "# y_pred_classes는 신경망이 예측한 레이블입니다.\n",
    "# 이 두 배열을 비교하여 평균적으로 얼마나 일치하는지 계산하여 정확도를 도출합니다.\n",
    "accuracy = np.mean(y_pred_classes.flatten() == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25860a0f-c997-4863-abde-40ba4138f5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
