오류역전파 알고리즘 실습

오류역전파 알고리즘의 기본 개념
목표: 신경망의 출력과 실제 값(목표 값) 사이의 오차를 줄이기 위해 신경망의 가중치를 조정합니다.
과정: 두 단계로 이루어집니다.
순전파(Forward Propagation): 입력 데이터가 신경망을 통과하여 출력을 생성합니다.
역전파(Backpropagation): 출력과 목표 값 사이의 오차를 계산하고, 이 오차를 신경망을 역으로 거슬러 올라가며 가중치에 대한 그래디언트를 계산합니다.
오류역전파의 구체적인 단계
순전파 단계:
입력 데이터가 네트워크의 각 층을 통과하며, 각 노드의 출력 값이 계산됩니다.
마지막 층에서의 출력 값이 생성됩니다.
손실 함수 계산:
신경망의 출력과 실제 목표 값 사이의 오차를 측정하는 손실 함수(Loss Function)를 계산합니다.
대표적인 손실 함수로는 평균 제곱 오차(Mean Squared Error)나 교차 엔트로피(Cross-Entropy) 등이 있습니다.
역전파 단계:
손실 함수의 그래디언트를 계산하여, 이를 신경망의 각 가중치에 대해 역방향으로 전파합니다.
이 과정에서 연쇄법칙(Chain Rule)을 사용하여 각 층의 가중치에 대한 손실 함수의 편미분을 계산합니다.
가중치 업데이트:
계산된 그래디언트를 사용하여 네트워크의 가중치를 업데이트합니다.
이 때, 학습률(Learning Rate)이라는 매개변수가 중요한 역할을 합니다. 학습률은 가중치 조정의 크기를 결정합니다.
반복:
이 과정을 여러 데이터에 대해 반복하여, 전체 데이터셋에 대한 학습이 이루어집니다.
중요성 및 장점
효율성: 오류역전파는 신경망의 가중치를 효율적으로 조정할 수 있도록 해줍니다.
범용성: 다양한 유형의 신경망(예: 완전연결층, 컨볼루션 신경망)에 적용될 수 있습니다.
주의점
기울기 소실/폭발 문제: 매우 깊은 네트워크에서는 기울기가 소실되거나 폭발할 수 있어 주의가 필요합니다.
적절한 하이퍼파라미터 설정: 학습률과 같은 하이퍼파라미터 설정이 중요합니다.
 

가중치와 오프셋 초기화 (Step 1)
모든 가중치와 노드 오프셋(또는 편향)을 작은 무작위 값으로 설정합니다.
입력과 원하는 출력 제시 (Step 2)
연속 값 입력 벡터와 원하는 출력을 제시합니다.
네트워크가 분류기로 사용될 경우, 원하는 출력은 대부분 0으로 설정되며, 해당 클래스의 입력에만 1로 설정됩니다.
입력은 매 시도마다 새로운 것일 수도 있고, 가중치가 안정될 때까지 학습 세트에서 순환적으로 제시될 수도 있습니다.
실제 출력 계산 (Step 3)
시그모이드 비선형 함수를 사용하여 출력을 계산합니다.
가중치 적응 (Step 4)
출력 노드에서 시작하여 첫 번째 은닉층까지 역순으로 작업하면서 가중치를 조정합니다.
이때 사용되는 수식은 가중치, 게인 항, 델타 항을 포함합니다.
출력 노드와 내부 은닉 노드에 따라 다른 조정 방식을 사용합니다.
가중치 변경에 모멘텀 항을 추가하여 가중치 변경을 평활화할 수 있습니다.
반복 (Step 5)
가중치가 안정될 때까지 위 단계들을 반복합니다.
